{
  "project": "TheOfficeU",
  "branchName": "ralph/remaining-milestones",
  "description": "Remaining milestones M3-M8: API polish, advanced agent interactions, persona evaluation & correction, experiment harness. See spec/plan/ for full details.",
  "userStories": [
    {
      "id": "US-001",
      "title": "S-3.2: OpenAPI Documentation",
      "description": "As a developer, I want the server to auto-generate an OpenAPI 3.1 spec from Zod schemas so every API route is documented and explorable. See spec/plan/milestone-3-api-layer-sse.md for full details.",
      "acceptanceCriteria": [
        "Install @asteasolutions/zod-to-openapi and @scalar/nextjs",
        "Create central OpenAPIRegistry in src/api/openapi.ts with all route schemas registered",
        "GET /api/openapi.json returns a valid OpenAPI 3.1 document",
        "GET /api/docs renders an interactive Scalar API explorer",
        "All existing routes (agents, memory, archival, channels, messages, reactions, runs, scheduled) appear in the spec with correct request/response schemas",
        "Spec validates via an OpenAPI validator (no schema errors)",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 1,
      "passes": false,
      "notes": "M3 story. All dependencies (M1, M2) are complete."
    },
    {
      "id": "US-002",
      "title": "S-3.3: Shim Cleanup",
      "description": "As a developer, I want all temporary shims from earlier stories removed. Review and clean up any temporary code added in M1-M3.",
      "acceptanceCriteria": [
        "No temporary shim code remains (search for TODO: cleanup markers)",
        "Remove dummy telemetry test button/route if no longer needed",
        "Clean up any hardcoded test data that should not be in production",
        "npm run build passes",
        "npm run lint passes",
        "All existing tests still pass",
        "Typecheck passes"
      ],
      "priority": 2,
      "passes": false,
      "notes": "M3 story. Depends on S-3.2 (US-001)."
    },
    {
      "id": "US-003",
      "title": "S-4.0: Agent-to-Agent DM Chains",
      "description": "As a developer, I want agents to automatically respond when they receive a DM from another agent, creating natural back-and-forth. Modify src/tools/send-message.ts and src/agents/orchestrator.ts. See spec/plan/milestone-4-advanced-interactions.md.",
      "acceptanceCriteria": [
        "When Agent A DMs Agent B via send_message tool, a run is enqueued for Agent B to respond",
        "Chain depth tracked in run metadata, MAX_CHAIN_DEPTH=3 enforced to prevent infinite loops",
        "Each agent in the chain sees the full DM history as context",
        "Unit tests for chain depth enforcement",
        "Integration test for a 2-agent exchange",
        "Sentry traces show the full chain with linked spans",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 3,
      "passes": false,
      "notes": "M4 story. Dependencies (M2, M5) are complete."
    },
    {
      "id": "US-004",
      "title": "S-4.1: Group Channel Response Ordering",
      "description": "As a developer, I want channel messages to trigger multiple agents with natural timing and ordered responses. Modify src/agents/orchestrator.ts for sequential processing. See spec/plan/milestone-4-advanced-interactions.md.",
      "acceptanceCriteria": [
        "Agents respond one at a time (not in parallel) to channel messages to avoid ordering issues",
        "Sequential processing does not add artificial delays",
        "Each agent sees previous agent responses in their conversation context",
        "Unit test for sequential ordering logic",
        "Sentry traces show sequential processing",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 4,
      "passes": false,
      "notes": "M4 story. Can be done in parallel with S-4.0."
    },
    {
      "id": "US-005",
      "title": "S-4.2: Skills System",
      "description": "As a developer, I want filesystem-based skills that provide character behavior knowledge. Create .skills/ directory with YAML+markdown skill files and API routes. See spec/plan/milestone-4-advanced-interactions.md.",
      "acceptanceCriteria": [
        "Create 6 skill files in .skills/: character-voice, conflict-resolution, meeting-dynamics, scenario-playbook, personality-drift-check, chat-etiquette",
        "Each skill file has YAML frontmatter (name, description) + markdown body",
        "Skills are loaded and injected into prompts by the Claude Agent SDK",
        "GET /api/skills returns list with name + description",
        "GET /api/skills/[name] returns full skill content",
        "Unit tests for skills API filesystem reading",
        "Sentry span for skill listing and fetch",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 5,
      "passes": false,
      "notes": "M4 story. Depends on S-4.0, S-4.1."
    },
    {
      "id": "US-006",
      "title": "S-4.3: Message Scheduler",
      "description": "As a developer, I want agents to autonomously initiate conversations on a schedule. Create scheduler loop, queries, instrumentation.ts startup hook, and API routes. See spec/plan/milestone-4-advanced-interactions.md.",
      "acceptanceCriteria": [
        "Create src/scheduler/loop.ts with polling loop checking every 10s for due scheduled messages",
        "Create src/db/queries/scheduler.ts with getDueMessages(), markFired(), createScheduled(), cancelScheduled()",
        "Create src/instrumentation.ts to start scheduler on Next.js server startup (Node.js runtime only)",
        "Create GET/POST /api/scheduled and DELETE /api/scheduled/[id] routes",
        "Fired messages marked as 'fired' to prevent re-execution",
        "Rate limit: max 1 scheduled fire per agent per 5 minutes",
        "Unit tests for scheduler logic, integration test for fire-and-mark flow",
        "Sentry spans for scheduler loop iterations and fires",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 6,
      "passes": false,
      "notes": "M4 story. Depends on S-4.2."
    },
    {
      "id": "US-007",
      "title": "S-6.0a: Evaluation Database Schema & Types",
      "description": "As a developer, I need the database schema and TypeScript types for the persona evaluation system. Create evaluation_runs and evaluation_scores tables, plus all shared types and Zod schemas. See spec/plan/milestone-6-persona-drift-measurement.md S-6.0.",
      "acceptanceCriteria": [
        "Create src/db/schema/evaluations.ts with evaluation_runs table (id, agent_id, status, dimensions, window_start, window_end, sample_size, overall_score, token_usage, timestamps) and evaluation_scores table (id, evaluation_run_id, dimension, proposition_id, score 0-9, reasoning, context_snippet)",
        "Create src/features/evaluation/types.ts with Proposition, PropositionResult, EvaluationDimension, EvaluationRun, EvaluationScore, TargetType, TrajectoryWindow, PreconditionFn types",
        "Create src/features/evaluation/schemas.ts with Zod schemas for proposition YAML format and evaluation API request/response",
        "Migration generated and applied — both tables queryable",
        "Create src/db/queries/evaluations.ts with createEvaluationRun(), recordScore(), getEvaluationRun(), listEvaluationRuns(), getAgentScoreHistory()",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 7,
      "passes": false,
      "notes": "M6 foundation story (part 1 of 3). Dependencies (M1, M2, M5) are complete."
    },
    {
      "id": "US-008",
      "title": "S-6.0b: Proposition YAML Loader & Template Engine",
      "description": "As a developer, I want to load proposition definitions from YAML files and fill template variables. Create the YAML loader and template interpolation. See spec/plan/milestone-6-persona-drift-measurement.md S-6.0.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/proposition-loader.ts that reads .yaml files from src/features/evaluation/propositions/",
        "Validates YAML files against Zod schema from US-007",
        "Supports template variables ({{agent_name}}, {{channel_name}}, {{action}}) filled at evaluation time via Mustache-style interpolation",
        "Supports optional inverted boolean per proposition — anti-patterns have raw score flipped (9 - raw)",
        "Supports include_personas flag, target_type (agent/environment), first_n/last_n trajectory windowing",
        "Supports hard evaluation mode (hard: true — 20% penalty for any detected flaw)",
        "Supports optional recommendations_for_improvement per proposition",
        "Create src/features/evaluation/propositions/README.md documenting the YAML format",
        "Create a sample proposition YAML file for testing (e.g., adherence/_default.yaml)",
        "Unit tests for YAML loading, template variable replacement, inverted score handling, hard mode penalty",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 8,
      "passes": false,
      "notes": "M6 foundation story (part 2 of 3). Depends on US-007."
    },
    {
      "id": "US-009",
      "title": "S-6.0c: Proposition Scoring Engine & Evaluation API",
      "description": "As a developer, I want the LLM judge scoring engine and evaluation API routes. Create proposition-engine.ts with score/check/batch modes and API routes. See spec/plan/milestone-6-persona-drift-measurement.md S-6.0.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/proposition-engine.ts with scoreProposition() returning integer 0-9, checkProposition() returning boolean, scorePropositions() for batch mode (up to 10 per call)",
        "Scoring rubric (0-9 with band descriptions matching TinyTroupe) included in every judge system prompt",
        "Single LLM judge prompt returns structured JSON with score, reasoning, confidence fields",
        "Supports double_check mode: judge asked to revise evaluation for stricter scoring",
        "Supports precondition functions gating when propositions apply",
        "Trajectory formatted as 'Agent acts: [MESSAGE]' for actions and '--> Agent: [STIMULUS]' for stimuli",
        "Create GET/POST /api/evaluations and GET /api/evaluations/[runId] routes",
        "Unit tests for LLM response parsing, batch grouping, scoring rubric presence, double-check flow, trajectory formatting, precondition gating, check() vs score() modes",
        "Sentry spans for evaluation runs and LLM judge calls",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 9,
      "passes": false,
      "notes": "M6 foundation story (part 3 of 3). Depends on US-008."
    },
    {
      "id": "US-010",
      "title": "S-6.1: Persona Adherence Scorer",
      "description": "As a developer, I want to score how well an agent's recent messages match its persona specification. See spec/plan/milestone-6-persona-drift-measurement.md S-6.1.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/scorers/adherence.ts with scoreAdherence(agentId, timeWindow, options)",
        "Create src/features/evaluation/propositions/adherence/_default.yaml with default adherence propositions",
        "Pulls recent send_message tool calls from run_messages for the target agent",
        "Samples up to 20 messages from the time window (random sample if more exist)",
        "Agent's system_prompt included as persona context in judge prompt (include_personas: true)",
        "Propositions from _default.yaml merged with agent-specific propositions",
        "Returns weighted-average score (0-9) with per-proposition breakdowns stored in evaluation_scores",
        "options.hard enables hard adherence mode: 20% penalty per flaw",
        "Add getRecentAgentMessages(agentId, window) to src/db/queries/evaluations.ts",
        "Unit tests with mocked LLM responses verifying score aggregation, message sampling, hard vs normal mode",
        "Sentry span wraps adherence scoring",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 10,
      "passes": false,
      "notes": "M6 scorer. Depends on US-009 (proposition engine)."
    },
    {
      "id": "US-011",
      "title": "S-6.2: Self-Consistency Scorer",
      "description": "As a developer, I want to measure whether an agent's current behavior is consistent with its own past behavior. See spec/plan/milestone-6-persona-drift-measurement.md S-6.2.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/scorers/consistency.ts with scoreConsistency(agentId, currentWindow, historicalWindow)",
        "Create src/features/evaluation/propositions/consistency/_default.yaml (include_personas: false)",
        "Pulls messages from both current and historical time windows",
        "Groups messages by channel to find comparable contexts",
        "Samples up to 10 message pairs for comparison",
        "LLM judge scores similarity (0-9) without access to persona spec (include_personas: false)",
        "Returns average consistency score with per-pair breakdowns",
        "Handles cold-start case: returns null if no historical messages exist",
        "Unit tests with mocked messages and LLM responses",
        "Sentry span wraps consistency scoring",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 11,
      "passes": false,
      "notes": "M6 scorer. Depends on US-009."
    },
    {
      "id": "US-012",
      "title": "S-6.3: Fluency Scorer",
      "description": "As a developer, I want to detect repetitive language patterns using n-gram analysis + LLM judge. See spec/plan/milestone-6-persona-drift-measurement.md S-6.3.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/scorers/fluency.ts with scoreFluency(agentId, timeWindow)",
        "Create src/features/evaluation/utils/ngram.ts with extractNgrams(text, n), computeOverlap(), computeCorpusRepetition(messages, n)",
        "Create src/features/evaluation/propositions/fluency/_default.yaml (include_personas: false)",
        "computeCorpusRepetition() returns 0-1 score (0 = all unique, 1 = all identical)",
        "scoreFluency() pulls recent messages, computes n-gram statistics, feeds both to LLM judge",
        "N-gram computation is pure TypeScript (no LLM call) — only final evaluation uses the judge",
        "Returns score (0-9) from LLM judge with reasoning",
        "Unit tests for n-gram extraction, overlap computation, and mocked LLM evaluation",
        "Sentry span wraps fluency scoring",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 12,
      "passes": false,
      "notes": "M6 scorer. Depends on US-009."
    },
    {
      "id": "US-013",
      "title": "S-6.4: Convergence/Divergence Scorer",
      "description": "As a developer, I want to measure whether agents in group conversations maintain individual voices. Environment-level scorer. See spec/plan/milestone-6-persona-drift-measurement.md S-6.4.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/scorers/convergence.ts with scoreConvergence(channelId, timeWindow)",
        "Create src/features/evaluation/utils/text-stats.ts with computeVocabularyStats() (unique word ratio, avg sentence length, punctuation density)",
        "Create src/features/evaluation/propositions/convergence/_default.yaml (include_personas: false, target_type: environment)",
        "Pulls messages from a channel and groups by agent",
        "Pairwise vocabulary similarity computed between all agent pairs",
        "LLM judge receives full conversation trajectory and evaluates divergence proposition",
        "Uses target_type: environment (all agents' messages as single context)",
        "Returns single divergence score (0-9) with reasoning plus per-agent-pair similarity metrics",
        "Unit tests for vocabulary stats, pairwise similarity, mocked LLM evaluation",
        "Sentry span wraps convergence scoring",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 13,
      "passes": false,
      "notes": "M6 scorer. Depends on US-009."
    },
    {
      "id": "US-014",
      "title": "S-6.6: Ideas Quantity Scorer",
      "description": "As a developer, I want to count distinct ideas proposed by agents in a conversation (integer count, not 0-9 score). See spec/plan/milestone-6-persona-drift-measurement.md S-6.6.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/scorers/ideas-quantity.ts with scoreIdeasQuantity(channelId, timeWindow)",
        "Create src/features/evaluation/propositions/ideas_quantity/_default.yaml (target_type: environment, include_personas: false)",
        "Pulls all messages from a channel within the time window",
        "LLM judge enumerates distinct ideas and returns integer count + list",
        "Duplicate/overlapping ideas collapsed by the judge",
        "Unit tests with mocked conversations and LLM responses",
        "Sentry span wraps ideas quantity scoring",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 14,
      "passes": false,
      "notes": "M6 scorer. Depends on US-009."
    },
    {
      "id": "US-015",
      "title": "S-6.5: Baseline Capture",
      "description": "As a developer, I want to capture baseline evaluation scores for all characters. Create CLI script, synthetic conversation generator, and baselines API. See spec/plan/milestone-6-persona-drift-measurement.md S-6.5.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/scripts/capture-baseline.ts CLI script (npm run eval:baseline)",
        "Create src/features/evaluation/scripts/sample-prompts.ts with 10 canned prompts",
        "Create src/features/evaluation/baseline.ts with captureBaseline(), getBaseline(), compareToBaseline()",
        "Create GET/POST /api/evaluations/baselines and GET /api/evaluations/baselines/[agentId] routes",
        "Add is_baseline boolean column to evaluation_runs table",
        "Runs all 5 scorers for specified agents",
        "For agents with <10 messages, generates synthetic conversations using sample prompts",
        "npm run eval:baseline captures baselines for all 16 agents (or subset via --agents michael,dwight)",
        "Baseline capture is idempotent (re-running replaces previous)",
        "Add eval:baseline script to package.json",
        "Unit tests for compareToBaseline() delta computation",
        "Sentry trace wraps baseline capture",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 15,
      "passes": false,
      "notes": "M6 story. Depends on all scorers (US-010 through US-014)."
    },
    {
      "id": "US-016",
      "title": "S-7.0a: Action Quality Check & Similarity",
      "description": "As a developer, I need the multi-dimension quality check and action similarity functions for the action correction gate. Create the per-dimension scoring (adherence, consistency, fluency, suitability) and Jaccard similarity check. See spec/plan/milestone-7-persona-drift-correction.md S-7.0.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/gates/action-correction.ts with checkActionQuality(agentId, messageText, conversationContext) returning per-dimension scores and overall pass/fail",
        "Create src/features/evaluation/gates/action-similarity.ts with computeActionSimilarity(proposedText, recentMessages) using Jaccard similarity (no LLM)",
        "Create src/features/evaluation/gates/types.ts with GateResult, DimensionResult, SimilarityResult, CorrectionAttempt, CorrectionStage, CorrectionConfig, GateStatistics types",
        "Four quality dimensions: persona_adherence (include_personas: true), self_consistency (include_personas: false), fluency (include_personas: false), suitability (include_personas: true, any-one-condition logic)",
        "Each dimension individually enabled/disabled, with its own score threshold (default 7)",
        "Action similarity: Jaccard of proposed message vs agent's last 5 messages, threshold default 0.6",
        "Quality checks use action-level trajectory windows: first_n: 5, last_n: 10",
        "Gate is a no-op when all quality checks disabled",
        "LLM judge calls have 5-second timeout per dimension; on timeout, that dimension passes (fail-open)",
        "Unit tests for each dimension, similarity check, timeout behavior, no-op when disabled",
        "Sentry spans for gate evaluation and each dimension check",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 16,
      "passes": false,
      "notes": "M7 story (part 1 of 2). Depends on US-009 (proposition engine)."
    },
    {
      "id": "US-017",
      "title": "S-7.0b: Two-Stage Correction Pipeline & send_message Integration",
      "description": "As a developer, I want the full correction pipeline (regeneration + direct correction) integrated into the send_message tool. Create direct correction, correction_logs table, statistics, BadActionInjector. See spec/plan/milestone-7-persona-drift-correction.md S-7.0.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/gates/direct-correction.ts with directCorrect(messageText, qualityFeedback, context) that extracts corrective rules and rewrites action text",
        "Create src/tests/factories/bad-action-injector.ts for testing the correction pipeline",
        "Add correction_logs table to src/db/schema/evaluations.ts (original text, per-dimension scores, reasoning, similarity score, attempt number, correction stage, outcome)",
        "Stage 1 (Regeneration): discard tool call, return feedback with per-dimension failures + recommendations_for_improvement + escalating 'be more radical' instruction. Max max_correction_attempts (default 2)",
        "Stage 2 (Direct Correction): LLM extracts corrective rules and directly rewrites. Max max_correction_attempts attempts. Re-evaluates through quality checks",
        "Both stages independently enabled via enable_regeneration and enable_direct_correction config flags",
        "Best-scoring attempt = SUM of all dimension scores across ALL stages. If continue_on_failure=true (default), best attempt committed",
        "Modify src/tools/send-message.ts to call gate before createMessage() when any quality check enabled",
        "Statistics tracking: total_actions, original_pass_count/rate, regeneration counts/rates/scores, direct_correction counts/rates/scores, forced_through_count, similarity_failure_count, per-dimension stats",
        "getGateStatistics(agentId, timeWindow) returns aggregated statistics",
        "Unit tests: single dimension fails with regeneration, direct correction after regeneration failure, both stages disabled, best-scoring selection, forced-through, statistics aggregation, BadActionInjector",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 17,
      "passes": false,
      "notes": "M7 story (part 2 of 2). Depends on US-016."
    },
    {
      "id": "US-018",
      "title": "S-7.1a: Intervention Framework Core",
      "description": "As a developer, I want a general-purpose intervention system with composable preconditions and effects. Create the Intervention class, precondition types, and chaining API. See spec/plan/milestone-7-persona-drift-correction.md S-7.1.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/interventions/intervention.ts with Intervention class and InterventionBatch",
        "Create src/features/evaluation/interventions/preconditions.ts with TextualPrecondition, FunctionalPrecondition, PropositionalPrecondition",
        "Create src/features/evaluation/interventions/types.ts with InterventionConfig, Precondition, PreconditionType, NudgeType, Nudge, InterventionContext, InterventionResult",
        "Three precondition types: textual (LLM Proposition.check()), functional (TypeScript function), propositional (M6 Proposition with optional threshold)",
        "All preconditions combined with AND logic",
        "Propositional with threshold: if score >= threshold, precondition is FALSE (inverted logic)",
        "Fluent chaining API: .setTextualPrecondition(), .setFunctionalPrecondition(), .setPropositionalPrecondition(), .setEffect() — all return this",
        "InterventionBatch.createForEach(agents) creates one per agent with shared template",
        "Add intervention_logs table to DB schema",
        "Unit tests for textual, functional, propositional preconditions, inverted threshold logic, batch creation, chaining API",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 18,
      "passes": false,
      "notes": "M7 story (part 1 of 2). Depends on US-009 (proposition engine)."
    },
    {
      "id": "US-019",
      "title": "S-7.1b: Built-in Interventions & Orchestrator Integration",
      "description": "As a developer, I want anti-convergence and variety interventions integrated into the orchestrator pipeline. Create built-in intervention types, nudge templates, and wire into orchestrator. See spec/plan/milestone-7-persona-drift-correction.md S-7.1.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/interventions/anti-convergence.ts with createAntiConvergenceIntervention(agentId, channelId)",
        "Create src/features/evaluation/interventions/variety-intervention.ts with createVarietyIntervention(agentId, channelId)",
        "Create src/features/evaluation/interventions/nudge-templates.ts with character-aware nudge templates (Michael: tell a story, Dwight: assert authority, Jim: witty observation, etc.)",
        "Anti-convergence: detects agreement patterns (LLM-based), injects character-aware disagreement nudge",
        "Variety: functional precondition (messageCount >= N) AND textual precondition, injects 'propose new ideas' thought",
        "Modify src/agents/prompt-builder.ts to accept optional interventions parameter, append nudge as '### Conversation Guidance' section",
        "Modify src/agents/orchestrator.ts to evaluate active interventions before agent invocation",
        "Nudges are transient (not stored in memory), only fire for channel messages (not DMs)",
        "Interventions evaluated once per orchestrator step, BEFORE agents act",
        "Unit tests for anti-convergence detection, variety intervention, nudge selection, prompt injection",
        "Sentry spans and logs for intervention evaluation and nudge injection",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 19,
      "passes": false,
      "notes": "M7 story (part 2 of 2). Depends on US-018."
    },
    {
      "id": "US-020",
      "title": "S-7.2: Repetition Suppression",
      "description": "As a developer, I want the system to detect and prevent step repetition by injecting the agent's recent messages as context with a 'vary your language' instruction. See spec/plan/milestone-7-persona-drift-correction.md S-7.2.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/interventions/repetition-suppression.ts with checkRepetition(agentId) and buildRepetitionContext(agentId, recentMessages)",
        "Fetches agent's last 5 sent messages across all channels from run_messages",
        "Computes 3-gram overlap using utilities from S-6.3 (src/features/evaluation/utils/ngram.ts)",
        "If overlap exceeds threshold (default 0.3), returns repeated n-grams and triggers suppression",
        "Suppression context includes recent messages list and 'vary your language' instruction with repeated n-grams listed as phrases to avoid",
        "Modify src/agents/prompt-builder.ts to accept optional repetitionContext parameter",
        "Modify src/agents/orchestrator.ts to call checkRepetition() before invocation",
        "Purely additive to system prompt — does not replace or modify existing sections",
        "No LLM call required — purely algorithmic n-gram analysis",
        "Unit tests for repetition detection, context building, prompt injection",
        "Sentry spans for repetition checks",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 20,
      "passes": false,
      "notes": "M7 story. Depends on US-017 (action gate) and US-019 (interventions). Uses ngram.ts from US-012."
    },
    {
      "id": "US-021",
      "title": "S-7.3: Correction Configuration & Cost Tracking",
      "description": "As a developer, I want per-agent configuration for all correction mechanisms and cost tracking. Create agent_evaluation_config table, config resolution, cost tracker, and API routes. See spec/plan/milestone-7-persona-drift-correction.md S-7.3.",
      "acceptanceCriteria": [
        "Create src/db/schema/evaluation-config.ts with agent_evaluation_config table including per-dimension gate toggles/thresholds, similarity config, regeneration/direct_correction toggles, max_correction_attempts, continue_on_failure, intervention toggles/thresholds, repetition config",
        "Create src/db/queries/evaluation-config.ts with getConfig(), updateConfig(), getDefaultConfig()",
        "Create src/features/evaluation/config.ts with resolveConfig(agentId) merging agent-specific overrides with defaults",
        "Create src/features/evaluation/cost-tracker.ts with trackCorrectionCost(), getCostSummary(), getTotalCostSummary()",
        "Create GET/PUT /api/evaluations/config and GET/PATCH /api/evaluations/config/[agentId] routes",
        "Create GET /api/evaluations/costs route (filterable by agentId and time window)",
        "Default config: all mechanisms disabled (matching TinyTroupe ENABLE_QUALITY_CHECKS=False default)",
        "All correction mechanisms read thresholds from config instead of hardcoded values",
        "Seed default configs for all 16 agents",
        "Unit tests for config resolution, cost aggregation",
        "Sentry spans for config loading",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 21,
      "passes": false,
      "notes": "M7 story. Depends on US-017, US-019, US-020."
    },
    {
      "id": "US-022",
      "title": "S-8.0: Proposition Library",
      "description": "As a developer, I want curated propositions for all 16 Office characters covering speech patterns, relationships, and behavioral traits. See spec/plan/milestone-8-evaluation-harness.md S-8.0.",
      "acceptanceCriteria": [
        "Create 16 adherence proposition YAML files in src/features/evaluation/propositions/adherence/ (one per character: michael, dwight, jim, pam, ryan, stanley, kevin, angela, oscar, andy, toby, creed, kelly, phyllis, meredith, darryl)",
        "Each character has 6-10 weighted propositions covering speech, relationships, and behavior",
        "Each proposition has unique id, natural language claim, and weight (0-1)",
        "_default.yaml contains universal propositions applied to all agents",
        "All YAML files validate against the Zod schema from US-008",
        "Each character has at least 1 anti-pattern proposition (inverted: true)",
        "Proposition loader correctly loads and merges agent-specific + default propositions",
        "Unit tests verify all YAML files load without errors and pass schema validation",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 22,
      "passes": false,
      "notes": "M8 story. Depends on US-008 (proposition loader). Can be done in parallel with M7 stories."
    },
    {
      "id": "US-023",
      "title": "S-8.1: Evaluation Harness CLI",
      "description": "As a developer, I want a CLI tool that runs propositions against conversations and produces a structured evaluation report. See spec/plan/milestone-8-evaluation-harness.md S-8.1.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/harness/cli.ts as entry point (npx tsx)",
        "Create src/features/evaluation/harness/runner.ts with runEvaluation(options)",
        "Create src/features/evaluation/harness/report.ts with generateReport(results) producing JSON report",
        "Create src/features/evaluation/harness/synthetic.ts for synthetic conversation generation",
        "Create src/features/evaluation/harness/mock-judge.ts with MockJudge class for deterministic CI testing",
        "CLI accepts --agents, --dimensions, --threshold, --mock-judge, --synthetic, --output flags",
        "Runs all five dimension scorers for specified agents",
        "JSON report has per-agent per-dimension scores and pass/fail with baselineDelta when baselines exist",
        "Exit code 0 when all pass, 1 when any fail",
        "--mock-judge mode returns pre-recorded scores without LLM calls",
        "Human-readable summary printed to stderr alongside JSON to stdout",
        "Add eval:run script to package.json",
        "Unit tests for report generation, CLI argument parsing, mock judge behavior",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 23,
      "passes": false,
      "notes": "M8 story. Depends on US-015 (baseline capture) and US-022 (proposition library)."
    },
    {
      "id": "US-024",
      "title": "S-8.2: Golden Baseline Storage & Comparison",
      "description": "As a developer, I want golden baseline scores stored as JSON files in the repo for regression comparison. See spec/plan/milestone-8-evaluation-harness.md S-8.2.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/baselines/ directory for golden baseline JSON files",
        "Create src/features/evaluation/harness/baseline-manager.ts with loadGoldenBaseline(), saveGoldenBaseline(), detectRegressions()",
        "Golden baselines stored as one JSON file per agent",
        "Regression detection: current score vs baseline, regression if drop exceeds delta (default 1.0 point)",
        "Add --update-baseline and --regression-delta flags to CLI",
        "Report includes regressions field listing which dimensions regressed and by how much",
        "Exit code 1 if any regressions detected",
        "Commit initial golden baselines for at least Michael, Dwight, Jim",
        "Unit tests for regression detection logic with various delta values",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 24,
      "passes": false,
      "notes": "M8 story. Depends on US-023."
    },
    {
      "id": "US-025",
      "title": "S-8.3: CI Integration",
      "description": "As a developer, I want the evaluation harness to run automatically on PRs that touch persona-related files. See spec/plan/milestone-8-evaluation-harness.md S-8.3.",
      "acceptanceCriteria": [
        "Create .github/workflows/persona-evaluation.yml GitHub Actions workflow",
        "Create src/features/evaluation/harness/ci-reporter.ts for markdown PR comment formatting",
        "Workflow triggers on PRs modifying persona-related files (evaluation/**, prompt-builder.ts, orchestrator.ts, send-message.ts, seed.ts, .skills/**)",
        "Workflow runs npm run eval:run -- --mock-judge in CI",
        "Posts PR comment with evaluation summary table (scores, regressions, pass/fail)",
        "PR check fails if any regressions exceed regression delta",
        "Workflow skips cleanly for PRs that don't touch persona files",
        "Workflow completes in under 60 seconds (mock-judge mode)",
        "Unit tests for CI reporter markdown generation",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 25,
      "passes": false,
      "notes": "M8 story. Depends on US-024."
    },
    {
      "id": "US-026",
      "title": "S-8.4: Agent Factory",
      "description": "As a developer, I want to programmatically generate large diverse agent populations for experiments (96-200 agents). Three-stage LLM pipeline with deterministic seeding. See spec/plan/milestone-8-evaluation-harness.md S-8.4.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/experiment/agent-factory.ts with AgentFactory class and generate(count, profile, options) method",
        "Create src/features/evaluation/experiment/population-profiles.ts with averageCustomer, difficultCustomer, politicalCompass profiles",
        "Create src/features/evaluation/experiment/persona-templates.ts for system prompt templates",
        "Create src/features/evaluation/experiment/types.ts with GeneratedPersona, PopulationProfile, FactoryOptions types",
        "Three-stage LLM pipeline: compute sampling dimensions, compute sampling plan, flatten to characteristics + generate personas",
        "Steps 1-2 cached for reuse",
        "Each persona includes name, age, gender, nationality, occupation, Big Five personality, style, goals, preferences, system_prompt, memory_blocks",
        "options.seed produces identical population composition across runs",
        "options.templateOnly generates from templates without LLM calls",
        "Parallel generation supported (concurrent persona generation)",
        "Unit tests: deterministic sampling with same seed, profile distributions, all required fields, parallel generation",
        "Sentry spans for persona generation",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 26,
      "passes": false,
      "notes": "M8 story. Depends on US-023 (harness CLI)."
    },
    {
      "id": "US-027",
      "title": "S-8.5: Scenario Library",
      "description": "As a developer, I want pre-defined experiment scenarios (brainstorming, debates) matching TinyTroupe's four experiments. Create scenario configs, facilitator, and step-based environment execution. See spec/plan/milestone-8-evaluation-harness.md S-8.5.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/experiment/scenario-library.ts with getScenario(id) and listScenarios()",
        "Create 4 scenario configs: brainstorming-average (Exp. 1), brainstorming-difficult-full (Exp. 2.1), brainstorming-difficult-variety (Exp. 2.2), debate-controversial (Exp. 3)",
        "Create src/features/evaluation/experiment/facilitator.ts for broadcasting prompts at step-indexed points",
        "Create src/features/evaluation/experiment/environment.ts with ExperimentEnvironment class implementing step-based execution",
        "Each scenario specifies: population_profile, agents_per_environment, total_environments, steps, facilitator_prompts, agent_order, treatment config, evaluation_dimensions",
        "Environment step execution: (1) evaluate interventions, (2) execute facilitator prompts, (3) agents act in configured order",
        "Sequential_random agent order: agents act one at a time in randomized order each step",
        "Treatment config specifies which correction mechanisms are active",
        "Unit tests: all scenarios load, config validation, facilitator prompt sequencing, environment step execution order",
        "Sentry spans for scenario loading and environment step execution",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 27,
      "passes": false,
      "notes": "M8 story. Depends on US-023 (harness CLI)."
    },
    {
      "id": "US-028",
      "title": "S-8.6: Experiment Runner & Statistical Testing",
      "description": "As a developer, I want an experiment runner that executes treatment vs. control groups with Welch's t-test and Cohen's d. See spec/plan/milestone-8-evaluation-harness.md S-8.6.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/experiment/cli.ts as experiment CLI entry point",
        "Create src/features/evaluation/experiment/runner.ts with ExperimentRunner class orchestrating full experiment lifecycle",
        "Create src/features/evaluation/experiment/environment-manager.ts for creating T/C environment pairs",
        "Create src/features/evaluation/experiment/statistical-testing.ts with welchTTest(), cohensD(), tDistributionCDF()",
        "Create src/features/evaluation/experiment/experiment-report.ts with generateExperimentReport() and DISPLAY_LABELS mapping",
        "Treatment group has correction mechanisms enabled per scenario; control group has all disabled",
        "Each agent evaluated on all scenario-specified dimensions after conversations complete",
        "Welch's t-test for each metric comparing T vs C, returning t-statistic, degrees of freedom, p-value",
        "Cohen's d effect size computed for each metric",
        "--seed option produces identical results across runs",
        "--runs N executes N independent runs and averages results",
        "--dry-run shows config without executing",
        "JSON report matches Table 1 structure: per-metric T mean(sd), C mean(sd), delta, p-value, significance",
        "Add experiment:run script to package.json",
        "Unit tests for Welch's t-test against known values, Cohen's d, result aggregation, deterministic seeding",
        "Sentry traces wrapping full experiment execution",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 28,
      "passes": false,
      "notes": "M8 story. Depends on US-026 (agent factory) and US-027 (scenario library)."
    },
    {
      "id": "US-029",
      "title": "S-8.7: Table 1 Reproduction",
      "description": "As a developer, I want to run all four TinyTroupe experiments and compare results against the paper's published values. See spec/plan/milestone-8-evaluation-harness.md S-8.7.",
      "acceptanceCriteria": [
        "Create src/features/evaluation/experiment/reproduce-table1.ts as CLI entry point",
        "Create src/features/evaluation/experiment/table1-reference.ts with hard-coded reference values from TinyTroupe Table 1",
        "Create src/features/evaluation/experiment/comparison-report.ts with generateComparisonReport(ours, theirs) for side-by-side comparison",
        "Runs all four experiments sequentially (or subset via --experiments)",
        "--scale factor allows reduced scale (e.g., --scale 0.1 for 10% agents/environments)",
        "Comparison report shows side-by-side: our T/C means vs paper's, delta direction match, p-value significance match",
        "Trend validation: for each significant result in paper, check if our delta has same sign",
        "Summary statistics: number of trends matched / total, overall reproduction score",
        "Human-readable comparison table printed to stderr matching Table 1 format with Match column",
        "--seed ensures reproducibility across runs",
        "Add experiment:table1 script to package.json",
        "Unit tests for reference value loading, comparison logic, trend matching",
        "Sentry trace wrapping full reproduction run",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 29,
      "passes": false,
      "notes": "M8 story. Depends on US-028 (experiment runner). This is the final story."
    }
  ]
}
